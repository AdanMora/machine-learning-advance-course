{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# We'll compare original, normalized, and standardized features\n",
    "print(\"Original data (first 5 rows):\")\n",
    "print(X.head())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X['sepal length (cm)'], X['sepal width (cm)'], c=iris.target, cmap='viridis', edgecolor='k', s=100)\n",
    "plt.title('Iris Dataset: Sepal Length vs Sepal Width')\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.colorbar(label='Species')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Normalization: [0, 1]\n",
    "minmax_scaler = MinMaxScaler().fit(X)\n",
    "X_norm = pd.DataFrame(minmax_scaler.transform(X), columns=X.columns)\n",
    "\n",
    "# ‚öôÔ∏è Standardization: mean=0, std=1\n",
    "std_scaler = StandardScaler().fit(X)\n",
    "X_std = pd.DataFrame(std_scaler.transform(X), columns=X.columns)\n",
    "\n",
    "# Plotting distributions for comparison\n",
    "feature = 'sepal length (cm)'\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Original\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(X[feature], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Original')\n",
    "\n",
    "# Normalized\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(X_norm[feature], bins=20, color='lightgreen', edgecolor='black')\n",
    "plt.title('Normalized (0-1)')\n",
    "\n",
    "# Standardized\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(X_std[feature], bins=20, color='salmon', edgecolor='black')\n",
    "plt.title('Standardized (mean=0, std=1)')\n",
    "\n",
    "plt.suptitle(f'Distribution of \"{feature}\"', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ourliers in Normalization and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 1000 values from a normal distribution (mean=50, std=10)\n",
    "data = np.random.normal(loc=50, scale=10, size=(1000, 1))\n",
    "\n",
    "# Add 5 extreme outliers\n",
    "outliers = np.array([[300], [310], [320], [330], [340]])\n",
    "data_with_outliers = np.vstack((data, outliers))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data_with_outliers, columns=[\"value\"])\n",
    "\n",
    "# Apply scalers\n",
    "minmax = MinMaxScaler().fit(df[[\"value\"]])\n",
    "standard = StandardScaler().fit(df[[\"value\"]])\n",
    "robust = RobustScaler().fit(df[[\"value\"]])\n",
    "\n",
    "df[\"minmax\"] = minmax.transform(df[[\"value\"]])\n",
    "df[\"standard\"] = standard.transform(df[[\"value\"]])\n",
    "df[\"robust\"] = robust.transform(df[[\"value\"]])\n",
    "\n",
    "# Plot all scalings\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Original data\n",
    "axes[0, 0].hist(df[\"value\"], bins=50, color='gray', edgecolor='black')\n",
    "axes[0, 0].set_title(\"Original Data\")\n",
    "\n",
    "# Min-Max Scaling\n",
    "axes[0, 1].hist(df[\"minmax\"], bins=50, color='lightblue', edgecolor='black')\n",
    "axes[0, 1].set_title(\"Min-Max Normalized\")\n",
    "\n",
    "# Standardization\n",
    "axes[1, 0].hist(df[\"standard\"], bins=50, color='orange', edgecolor='black')\n",
    "axes[1, 0].set_title(\"Standardized\")\n",
    "\n",
    "# Robust Scaling\n",
    "axes[1, 1].hist(df[\"robust\"], bins=50, color='green', edgecolor='black')\n",
    "axes[1, 1].set_title(\"Robust Scaled\")\n",
    "\n",
    "# Layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Applying Normalizaction and Standardization in a ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# üîç Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# üß™ Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "breast_cancer_df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "breast_cancer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Logistic Regression WITHOUT scaling\n",
    "# -----------------------------\n",
    "model_no_scaling = LogisticRegression(max_iter=1000, penalty=None)\n",
    "model_no_scaling.fit(X_train, y_train)\n",
    "y_pred_ns = model_no_scaling.predict(X_test)\n",
    "acc_ns = accuracy_score(y_test, y_pred_ns)\n",
    "\n",
    "# -----------------------------\n",
    "# Logistic Regression WITH StandardScaler in Pipeline\n",
    "# -----------------------------\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logreg\", LogisticRegression(max_iter=1000, penalty=None))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_scaled = pipeline.predict(X_test)\n",
    "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "# -----------------------------\n",
    "# Logistic Regression WITH Normalization in Pipeline\n",
    "# -----------------------------\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"logreg\", LogisticRegression(max_iter=1000, penalty=None))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_norm = pipeline.predict(X_test)\n",
    "acc_norm = accuracy_score(y_test, y_pred_norm)\n",
    "\n",
    "# -----------------------------\n",
    "# Compare results\n",
    "# -----------------------------\n",
    "print(f\"Accuracy without standardization: {acc_ns:.4f}\")\n",
    "print(f\"Accuracy with standardization:    {acc_scaled:.4f}\")\n",
    "print(f\"Accuracy with normalization:    {acc_norm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,          # Total number of samples\n",
    "    n_features=10,           # Number of features\n",
    "    n_informative=5,        # Number of informative features\n",
    "    n_redundant=2,           # Number of redundant features\n",
    "    n_classes=5,            # Number of classes\n",
    "    n_clusters_per_class=1,  # Clusters per class\n",
    "    weights=[0.13, 0.5, 0.1, 0.07, 0.2],  # Imbalanced class weights\n",
    "    flip_y=0,                # No label noise\n",
    "    random_state=42          # Reproducibility\n",
    ")\n",
    "\n",
    "# Check class distribution\n",
    "class_counts = Counter(y)\n",
    "print(\"Class distribution:\", class_counts)\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.bar(class_counts.keys(), class_counts.values())\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Split the dataset without stratification\n",
    "X_train_ns, X_test_ns, y_train_ns, y_test_ns = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=None\n",
    ")\n",
    "\n",
    "# Split the dataset with stratification\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y\n",
    ")\n",
    "\n",
    "# Function to calculate class distribution and percentages\n",
    "def calculate_distribution(y_data):\n",
    "    class_counts = Counter(y_data)\n",
    "    total = sum(class_counts.values())\n",
    "    percentages = {cls: (count / total) * 100 for cls, count in class_counts.items()}\n",
    "    return pd.DataFrame({\n",
    "        \"Class\": list(class_counts.keys()),\n",
    "        \"Count\": list(class_counts.values()),\n",
    "        \"Percentage\": list(percentages.values())\n",
    "    }).sort_values(by=\"Class\").reset_index(drop=True)\n",
    "\n",
    "# Calculate distributions\n",
    "df_train_ns = calculate_distribution(y_train_ns)\n",
    "df_test_ns = calculate_distribution(y_test_ns)\n",
    "df_train_s = calculate_distribution(y_train_s)\n",
    "df_test_s = calculate_distribution(y_test_s)\n",
    "\n",
    "# Combine results into a single DataFrame for comparison\n",
    "comparison_df = pd.concat([\n",
    "    df_train_ns.rename(columns={\"Count\": \"Train Count (NS)\", \"Percentage\": \"Train % (NS)\"}),\n",
    "    df_test_ns.rename(columns={\"Count\": \"Test Count (NS)\", \"Percentage\": \"Test % (NS)\"}),\n",
    "    df_train_s.rename(columns={\"Count\": \"Train Count (S)\", \"Percentage\": \"Train % (S)\"}),\n",
    "    df_test_s.rename(columns={\"Count\": \"Test Count (S)\", \"Percentage\": \"Test % (S)\"})\n",
    "], axis=1)\n",
    "\n",
    "# Display the comparison DataFrame\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
